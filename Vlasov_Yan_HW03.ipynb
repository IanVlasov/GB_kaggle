{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from src.validation import make_cross_validation\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './'\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'assignment_train.csv')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'assignment_test.csv')\n",
    "TARGET_COLUMN = 'isFraud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 0\n",
    "\n",
    "выбрать любую модель машнного обучения и зафиксировать любой тип валидации. Обучить базовую модель и зафиксировать базовое качество модели. В каждом следующем задании нужно будет обучить выбранную модель и оценивать ее качество на зафиксированной схеме валидации. После каждого задания, требуется сделать вывод о достигаемом качестве модели, по сравнению с качестом из предыдущего шага.\n",
    "\n",
    "**Комментрарий**\n",
    "\n",
    "В задании 2 у меня лучше других показала себя схема кроссвалидации на 5 фолдов. Использованная модель: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = train.select_dtypes(\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[cat_features] = test[cat_features].astype(str)\n",
    "train[cat_features] = train[cat_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_x = test.drop([\"TransactionID\", TARGET_COLUMN], axis=1)\n",
    "public_test_y = test[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"configs/catboost_config.yaml\"\n",
    "with open(config_name) as cfg:\n",
    "        model_params = yaml.load(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8851, valid-score = 0.8384\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8826, valid-score = 0.8569\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8764, valid-score = 0.8811\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8791, valid-score = 0.8597\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8779, valid-score = 0.8856\n",
      "=====================================================================\n",
      "CV-results train: 0.8802 +/- 0.003\n",
      "CV-results valid: 0.8644 +/- 0.017\n",
      "OOF-score = 0.8627\n"
     ]
    }
   ],
   "source": [
    "cv_strategy = KFold(n_splits=5, random_state=27)\n",
    "\n",
    "estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "    train.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "    metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8599287618923193\n"
     ]
    }
   ],
   "source": [
    "y_pred_lb = 0\n",
    "for estimator in estimators:\n",
    "    y_pred_lb += estimator.predict_proba(public_test_x)[:, 1]\n",
    "    \n",
    "y_pred_lb = y_pred_lb / 5\n",
    "\n",
    "public_test_score = roc_auc_score(public_test_y, \n",
    "                                  y_pred_lb)\n",
    "\n",
    "print(public_test_score)\n",
    "\n",
    "BASE_OOF_SCORE = oof_score\n",
    "BASE_PUBLIC_SCORE = public_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SCORES_FOR_TTEST = np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    cv_strategy = KFold(n_splits=5, random_state=i)\n",
    "\n",
    "    estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "        train.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "        metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features, log=False\n",
    "    )\n",
    "    \n",
    "    BASE_SCORES_FOR_TTEST = np.append(BASE_SCORES_FOR_TTEST, fold_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "Базовое качество модели при фиксированной схеме валидации: 0.8599"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "признак TransactionDT - это смещение в секундах относительно базовой даты. Базовая дата - 2017-12-01, преобразовать признак TransactionDT в datetime, прибавив к базовой дате исходное значение признака. Из полученного признака выделить год, месяц, день недели, час, день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(dataset, datetime_feature, start_date=None):\n",
    "    \"\"\"Функция создает в *dataset* признаки на основе даты:\n",
    "    год, месяц, номер недели, день месяца, день недели, час \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    dataset: pd.Dataframe\n",
    "        датасет, в который будут добавлены новые столбцы\n",
    "    \n",
    "    datetime_feature: str\n",
    "        название признака, содержащего дату в формате timestamp в *dataset*\n",
    "        \n",
    "    start_date: datetime, optional\n",
    "        начальная дата\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    dataset: pd.Dataframe\n",
    "        копия исходного датасета с добавленными признаками\n",
    "    \"\"\"\n",
    "    \n",
    "    if start_date:\n",
    "        if not isinstance(start_date, datetime.datetime):\n",
    "            msg = \"invalid type for start_date\"\n",
    "            raise TypeError(msg)\n",
    "            \n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    \n",
    "    if start_date:\n",
    "        timedeltas = dataset[datetime_feature].apply(lambda x: datetime.timedelta(seconds=int(x)))\n",
    "        actual_dates = timedeltas + start_date\n",
    "        \n",
    "    else:\n",
    "        actual_dates = dataset[datetime_feature].apply(datetime.datetime.fromtimestamp())\n",
    "        \n",
    "    dataset[\"year\"] = actual_dates.dt.year\n",
    "    dataset[\"month\"] = actual_dates.dt.month\n",
    "    dataset[\"week_of_year\"] = actual_dates.dt.weekofyear\n",
    "    dataset[\"day_of_year\"] = actual_dates.dt.dayofyear\n",
    "    dataset[\"day_of_month\"] = actual_dates.dt.day\n",
    "    dataset[\"day_of_week\"] = actual_dates.dt.weekday\n",
    "    dataset[\"hour\"] = actual_dates.dt.hour\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2017, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wdates = create_date_features(train, 'TransactionDT', start_date)\n",
    "public_test_x_wdates = create_date_features(public_test_x, 'TransactionDT', start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ... V337  V338  V339  year  month  \\\n",
       "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN  2017     12   \n",
       "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN  2017     12   \n",
       "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN  2017     12   \n",
       "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN  2017     12   \n",
       "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0  2017     12   \n",
       "\n",
       "  week_of_year day_of_year  day_of_month  day_of_week  hour  \n",
       "0           48         336             2            5     0  \n",
       "1           48         336             2            5     0  \n",
       "2           48         336             2            5     0  \n",
       "3           48         336             2            5     0  \n",
       "4           48         336             2            5     0  \n",
       "\n",
       "[5 rows x 401 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wdates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.889, valid-score = 0.8374\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8807, valid-score = 0.8586\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8782, valid-score = 0.883\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8832, valid-score = 0.8636\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.88, valid-score = 0.8865\n",
      "=====================================================================\n",
      "CV-results train: 0.8822 +/- 0.004\n",
      "CV-results valid: 0.8658 +/- 0.018\n",
      "OOF-score = 0.864\n"
     ]
    }
   ],
   "source": [
    "estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "    train_wdates.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "    metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8610899625267636"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lb = 0\n",
    "for estimator in estimators:\n",
    "    y_pred_lb += estimator.predict_proba(public_test_x_wdates)[:, 1]\n",
    "    \n",
    "y_pred_lb = y_pred_lb / 5\n",
    "\n",
    "public_test_score = roc_auc_score(public_test_y, \n",
    "                                  y_pred_lb)\n",
    "public_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    cv_strategy = KFold(n_splits=5, random_state=i)\n",
    "\n",
    "    estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "        train_wdates.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "        metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features, log=False\n",
    "    )\n",
    "    \n",
    "    scores = np.append(scores, fold_valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF_SCORE изменился на 0.0014\n",
      "Public score изменился на 0.0012\n"
     ]
    }
   ],
   "source": [
    "print(f'OOF_SCORE изменился на {oof_score - BASE_OOF_SCORE:.4f}')\n",
    "print(f'Public score изменился на {public_test_score - BASE_PUBLIC_SCORE:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=6.4754572833311075, pvalue=4.310757210152398e-08)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_rel(scores, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "с признаками даты результаты и на кроссвалидации и на паблик тесте стали незначительно лучше. Вместе с тем доверительные интервалы увеличились на 0.001. При исследовани датасета было показано, что транзакции имеют неярко выраженную сезонность, возможно, это и повлияло на улучшенние прогнозов. По значению t-теста также видно, что улучшение прогнозов статистически значимо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "сгруппировать данные по card1 и посчитать среднюю сумму транзакции. Добавить в качестве признака в набор данных. Посчитать разницу между суммой транзакцией пользователя и средней суммой транзакции по данному типу card1. Построить отношение этих признаков. Повторить процедуру для всех card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_aggs(data: pd.DataFrame,\n",
    "                          groupby_id: str,\n",
    "                          aggs: dict,\n",
    "                          prefix: Optional[str] = None,\n",
    "                          suffix: Optional[str] = None,\n",
    "                          ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Построение агрегаций для числовых признаков.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.core.frame.DataFrame\n",
    "        Выборка для построения агрегаций.\n",
    "\n",
    "    groupby_id: str\n",
    "        Название ключа, по которому нужно произвести группировку.\n",
    "\n",
    "    aggs: dict\n",
    "        Словарь с названием признака и списка функций.\n",
    "        Ключ словаря - название признака, который используется для\n",
    "        вычисления агрегаций, значение словаря - список с названием\n",
    "        функций для вычисления агрегаций.\n",
    "\n",
    "    prefix: str, optional, default = None\n",
    "        Префикс для названия признаков.\n",
    "        Опциональный параметр, по умолчанию, не используется.\n",
    "\n",
    "    suffix: str, optional, default = None\n",
    "        Суффикс для названия признаков.\n",
    "        Опциональный параметр, по умолчанию, не используется.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: pandas.core.frame.DataFrame\n",
    "        Выборка с рассчитанными агрегациями.\n",
    "\n",
    "    \"\"\"\n",
    "    if not prefix:\n",
    "        prefix = \"\"\n",
    "    if not suffix:\n",
    "        suffix = \"\"\n",
    "\n",
    "    data_grouped = data.groupby(groupby_id)\n",
    "    stats = data_grouped.agg(aggs)\n",
    "    stats.columns = [f\"{prefix}{feature}_{stat}{suffix}\".upper() for feature, stat in stats]\n",
    "    stats = stats.reset_index()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "CARD1\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8866, valid-score = 0.8379\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8801, valid-score = 0.8569\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8773, valid-score = 0.8786\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8825, valid-score = 0.8677\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8779, valid-score = 0.8816\n",
      "=====================================================================\n",
      "CV-results train: 0.8809 +/- 0.003\n",
      "CV-results valid: 0.8645 +/- 0.016\n",
      "OOF-score = 0.8627\n",
      "Public test score: 0.8607\n",
      "OOF_SCORE изменился на 0.0000\n",
      "Public score изменился на 0.0007\n",
      "Ttest: Ttest_relResult(statistic=0.3112118563765284, pvalue=0.7569597562868753)\n",
      "****************************************************************************************************\n",
      "CARD2\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8892, valid-score = 0.8429\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8809, valid-score = 0.857\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8783, valid-score = 0.8778\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8839, valid-score = 0.8647\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8793, valid-score = 0.8824\n",
      "=====================================================================\n",
      "CV-results train: 0.8823 +/- 0.004\n",
      "CV-results valid: 0.865 +/- 0.014\n",
      "OOF-score = 0.8633\n",
      "Public test score: 0.8601\n",
      "OOF_SCORE изменился на 0.0006\n",
      "Public score изменился на 0.0002\n",
      "Ttest: Ttest_relResult(statistic=1.2099292606499656, pvalue=0.23211084885724917)\n",
      "****************************************************************************************************\n",
      "CARD3\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8876, valid-score = 0.8409\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8834, valid-score = 0.8629\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8783, valid-score = 0.8831\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8852, valid-score = 0.8654\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8818, valid-score = 0.8874\n",
      "=====================================================================\n",
      "CV-results train: 0.8833 +/- 0.003\n",
      "CV-results valid: 0.8679 +/- 0.017\n",
      "OOF-score = 0.8661\n",
      "Public test score: 0.8641\n",
      "OOF_SCORE изменился на 0.0034\n",
      "Public score изменился на 0.0041\n",
      "Ttest: Ttest_relResult(statistic=13.789154567264191, pvalue=1.7180529118770699e-18)\n",
      "****************************************************************************************************\n",
      "CARD4\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.887, valid-score = 0.8395\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8811, valid-score = 0.8576\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8775, valid-score = 0.8814\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8818, valid-score = 0.8628\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8779, valid-score = 0.8843\n",
      "=====================================================================\n",
      "CV-results train: 0.881 +/- 0.003\n",
      "CV-results valid: 0.8651 +/- 0.016\n",
      "OOF-score = 0.863\n",
      "Public test score: 0.8585\n",
      "OOF_SCORE изменился на 0.0003\n",
      "Public score изменился на -0.0014\n",
      "Ttest: Ttest_relResult(statistic=3.664861864794342, pvalue=0.0006077946213053601)\n",
      "****************************************************************************************************\n",
      "CARD5\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8848, valid-score = 0.8336\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8822, valid-score = 0.8581\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8733, valid-score = 0.8808\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8791, valid-score = 0.8629\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8795, valid-score = 0.8867\n",
      "=====================================================================\n",
      "CV-results train: 0.8798 +/- 0.004\n",
      "CV-results valid: 0.8644 +/- 0.019\n",
      "OOF-score = 0.8622\n",
      "Public test score: 0.8619\n",
      "OOF_SCORE изменился на -0.0005\n",
      "Public score изменился на 0.0020\n",
      "Ttest: Ttest_relResult(statistic=0.12866446384447436, pvalue=0.8981498133379213)\n",
      "****************************************************************************************************\n",
      "CARD6\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8882, valid-score = 0.8383\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8787, valid-score = 0.8552\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8753, valid-score = 0.8788\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8806, valid-score = 0.8655\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8772, valid-score = 0.8834\n",
      "=====================================================================\n",
      "CV-results train: 0.88 +/- 0.004\n",
      "CV-results valid: 0.8642 +/- 0.016\n",
      "OOF-score = 0.8625\n",
      "Public test score: 0.8601\n",
      "OOF_SCORE изменился на -0.0002\n",
      "Public score изменился на 0.0002\n",
      "Ttest: Ttest_relResult(statistic=-0.3044967184306636, pvalue=0.7620389985961151)\n"
     ]
    }
   ],
   "source": [
    "aggs = {'TransactionAmt': [np.mean]}\n",
    "\n",
    "for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    stats = create_numerical_aggs(all_data, groupby_id=feature, aggs=aggs)\n",
    "    stat_name = stats.columns[-1]\n",
    "    \n",
    "    train_agg = train.merge(stats, how='left', on=feature)\n",
    "    public_test_x_agg = public_test_x.merge(stats, how='left', on=feature)\n",
    "    \n",
    "    train_agg[f'TRANSACTIONAMT - {stat_name}'] = train_agg['TransactionAmt'] - train_agg[stat_name]\n",
    "    public_test_x_agg[f'TRANSACTIONAMT - {stat_name}'] = public_test_x_agg['TransactionAmt'] -\\\n",
    "        public_test_x_agg[stat_name]\n",
    "    \n",
    "    train_agg[f'TRANSACTIONAMT / {stat_name}'] = train_agg['TransactionAmt'] / train_agg[stat_name]\n",
    "    public_test_x_agg[f'TRANSACTIONAMT / {stat_name}'] = public_test_x_agg['TransactionAmt'] /\\\n",
    "        public_test_x_agg[stat_name]\n",
    "    \n",
    "    print('*'*100)\n",
    "    print(feature.upper())\n",
    "    print('*'*100)\n",
    "    \n",
    "    estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "        train_agg.drop(columns=['TransactionID', TARGET_COLUMN]), train_agg[TARGET_COLUMN], model, \n",
    "        metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    y_pred_lb = 0\n",
    "    for estimator in estimators:\n",
    "        y_pred_lb += estimator.predict_proba(public_test_x_agg)[:, 1]\n",
    "\n",
    "    y_pred_lb = y_pred_lb / 5\n",
    "\n",
    "    public_test_score = roc_auc_score(public_test_y, \n",
    "                                      y_pred_lb)\n",
    "    \n",
    "    scores = np.array([])\n",
    "\n",
    "    for i in range(10):\n",
    "        cv_strategy = KFold(n_splits=5, random_state=i)\n",
    "\n",
    "        estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "            train_agg.drop(columns=['TransactionID', TARGET_COLUMN]), train_agg[TARGET_COLUMN], model, \n",
    "            metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features, log=False\n",
    "        )\n",
    "\n",
    "        scores = np.append(scores, fold_valid_scores)\n",
    "        \n",
    "    print(f'Public test score: {public_test_score:.4f}')\n",
    "    \n",
    "    print(f'OOF_SCORE изменился на {oof_score - BASE_OOF_SCORE:.4f}')\n",
    "    print(f'Public score изменился на {public_test_score - BASE_PUBLIC_SCORE:.4f}')\n",
    "    print(f'Ttest: {ttest_rel(scores, BASE_SCORES_FOR_TTEST)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "Значимые изменения показали только новые признаки на основе признаков card3 и card4. Card3 показал значительные улучшения, случай card4 интереснее тем, что предсказание на тесте потеряло точность (но в пределах доверительного интервала), а вот значение статистики говорит о том, что вклад новых признаков положительный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "преобразовать признаки card_1 - card_6 с помощью Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оформил в функцию сравнение двух моделей и вывод статистики\n",
    "def compare_with_base(train, test, model, base_oof, base_public, base_ttest):\n",
    "    cv_strategy = KFold(n_splits=5, random_state=27)\n",
    "    \n",
    "    estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "        train.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "        metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features\n",
    "    )\n",
    "\n",
    "    y_pred_lb = 0\n",
    "    for estimator in estimators:\n",
    "        y_pred_lb += estimator.predict_proba(test)[:, 1]\n",
    "\n",
    "    y_pred_lb = y_pred_lb / 5\n",
    "\n",
    "    public_test_score = roc_auc_score(public_test_y, \n",
    "                                      y_pred_lb)\n",
    "\n",
    "    scores = np.array([])\n",
    "\n",
    "    for i in range(10):\n",
    "        cv_strategy = KFold(n_splits=5, random_state=i)\n",
    "\n",
    "        estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "            train.drop(columns=['TransactionID', TARGET_COLUMN]), train[TARGET_COLUMN], model, \n",
    "            metric=roc_auc_score, cv_strategy=cv_strategy, cat_features=cat_features, log=False\n",
    "        )\n",
    "\n",
    "        scores = np.append(scores, fold_valid_scores)\n",
    "\n",
    "    print(f'Public test score: {public_test_score:.4f}')\n",
    "\n",
    "    print(f'OOF_SCORE изменился на {oof_score - BASE_OOF_SCORE:.4f}')\n",
    "    print(f'Public score изменился на {public_test_score - BASE_PUBLIC_SCORE:.4f}')\n",
    "    print(f'Ttest: {ttest_rel(scores, BASE_SCORES_FOR_TTEST)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "CARD1\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.888, valid-score = 0.8386\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8796, valid-score = 0.8569\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8769, valid-score = 0.8823\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.882, valid-score = 0.8607\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.879, valid-score = 0.886\n",
      "=====================================================================\n",
      "CV-results train: 0.8811 +/- 0.004\n",
      "CV-results valid: 0.8649 +/- 0.017\n",
      "OOF-score = 0.8632\n",
      "Public test score: 0.8616\n",
      "OOF_SCORE изменился на 0.0005\n",
      "Public score изменился на 0.0016\n",
      "Ttest: Ttest_relResult(statistic=8.637651497634545, pvalue=2.0577869071666217e-11)\n",
      "****************************************************************************************************\n",
      "CARD2\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8866, valid-score = 0.834\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8825, valid-score = 0.8596\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8769, valid-score = 0.8814\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.879, valid-score = 0.8578\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8795, valid-score = 0.8876\n",
      "=====================================================================\n",
      "CV-results train: 0.8809 +/- 0.003\n",
      "CV-results valid: 0.8641 +/- 0.019\n",
      "OOF-score = 0.8618\n",
      "Public test score: 0.8602\n",
      "OOF_SCORE изменился на -0.0009\n",
      "Public score изменился на 0.0003\n",
      "Ttest: Ttest_relResult(statistic=-0.7284604096617339, pvalue=0.4697990472173431)\n",
      "****************************************************************************************************\n",
      "CARD3\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8857, valid-score = 0.8383\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8823, valid-score = 0.8604\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8749, valid-score = 0.8835\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8842, valid-score = 0.8629\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8778, valid-score = 0.8833\n",
      "=====================================================================\n",
      "CV-results train: 0.881 +/- 0.004\n",
      "CV-results valid: 0.8657 +/- 0.017\n",
      "OOF-score = 0.8634\n",
      "Public test score: 0.8596\n",
      "OOF_SCORE изменился на 0.0007\n",
      "Public score изменился на -0.0003\n",
      "Ttest: Ttest_relResult(statistic=4.261319357582482, pvalue=9.200333229763965e-05)\n",
      "****************************************************************************************************\n",
      "CARD4\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.886, valid-score = 0.8402\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8799, valid-score = 0.8568\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8749, valid-score = 0.8806\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8843, valid-score = 0.8629\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8762, valid-score = 0.8857\n",
      "=====================================================================\n",
      "CV-results train: 0.8803 +/- 0.004\n",
      "CV-results valid: 0.8652 +/- 0.016\n",
      "OOF-score = 0.8633\n",
      "Public test score: 0.8581\n",
      "OOF_SCORE изменился на 0.0006\n",
      "Public score изменился на -0.0019\n",
      "Ttest: Ttest_relResult(statistic=4.358565373775681, pvalue=6.691732014923085e-05)\n",
      "****************************************************************************************************\n",
      "CARD5\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8876, valid-score = 0.8406\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8838, valid-score = 0.8582\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8728, valid-score = 0.8803\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8801, valid-score = 0.858\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.878, valid-score = 0.8837\n",
      "=====================================================================\n",
      "CV-results train: 0.8805 +/- 0.005\n",
      "CV-results valid: 0.8642 +/- 0.016\n",
      "OOF-score = 0.8618\n",
      "Public test score: 0.8607\n",
      "OOF_SCORE изменился на -0.0009\n",
      "Public score изменился на 0.0007\n",
      "Ttest: Ttest_relResult(statistic=-0.8491892385051473, pvalue=0.3999058707718377)\n",
      "****************************************************************************************************\n",
      "CARD6\n",
      "****************************************************************************************************\n",
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.889, valid-score = 0.8385\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8805, valid-score = 0.8573\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8771, valid-score = 0.8805\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8801, valid-score = 0.8585\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8776, valid-score = 0.8845\n",
      "=====================================================================\n",
      "CV-results train: 0.8809 +/- 0.004\n",
      "CV-results valid: 0.8639 +/- 0.017\n",
      "OOF-score = 0.8618\n",
      "Public test score: 0.8601\n",
      "OOF_SCORE изменился на -0.0009\n",
      "Public score изменился на 0.0002\n",
      "Ttest: Ttest_relResult(statistic=-5.543708762848295, pvalue=1.1713022868408678e-06)\n"
     ]
    }
   ],
   "source": [
    "train_freq = copy.deepcopy(train)\n",
    "public_test_x_freq = copy.deepcopy(public_test_x)\n",
    "\n",
    "for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    freq_encoder = all_data[feature].value_counts(normalize=True)\n",
    "    train_freq[f'{feature}_freq_enc'] = train_freq[feature].map(freq_encoder)\n",
    "    public_test_x_freq[f'{feature}_freq_enc'] = public_test_x_freq[feature].map(freq_encoder)\n",
    "    \n",
    "    train_freq.drop(columns=[feature])\n",
    "    public_test_x_freq.drop(columns=[feature])\n",
    "    \n",
    "    print('*'*100)\n",
    "    print(feature.upper())\n",
    "    print('*'*100)\n",
    "    \n",
    "    compare_with_base(train_freq, public_test_x_freq, model, BASE_OOF_SCORE, \n",
    "                      BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "Значимые изменения показали преобразования признаков CARD1, CARD3, CARD4 и CARD6. Но в случае с CARD6 прогнозы ухудшились, поэтому это преобразование не считается удачным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "\n",
    "преобразовать признак TransactionAmt в логариф признака, выделить дробную часть и целую часть в отдельные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.882, valid-score = 0.8336\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8804, valid-score = 0.8577\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8752, valid-score = 0.8766\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8782, valid-score = 0.8606\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.879, valid-score = 0.8879\n",
      "=====================================================================\n",
      "CV-results train: 0.879 +/- 0.002\n",
      "CV-results valid: 0.8633 +/- 0.018\n",
      "OOF-score = 0.8614\n",
      "Public test score: 0.8590\n",
      "OOF_SCORE изменился на -0.0013\n",
      "Public score изменился на -0.0009\n",
      "Ttest: Ttest_relResult(statistic=-2.499424241475067, pvalue=0.01583853157848215)\n"
     ]
    }
   ],
   "source": [
    "train_log = copy.deepcopy(train)\n",
    "public_test_x_log = copy.deepcopy(public_test_x)\n",
    "\n",
    "for data in (train_log, public_test_x_log):\n",
    "    data['TransactionAmt'] = data['TransactionAmt'].apply(np.log)\n",
    "    data.rename(columns={'TransactionAmt': 'LogTransAmt'}, inplace=True)\n",
    "    data['LogTransAmt_frac'] = data['LogTransAmt'].map(lambda x: np.modf(x)[0])\n",
    "    data['LogTransAmt_int'] = data['LogTransAmt'].map(lambda x: np.modf(x)[1])\n",
    "    \n",
    "compare_with_base(train_log, public_test_x_log, model, BASE_OOF_SCORE, \n",
    "                  BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "Нулевая гипотеза о равенстве распределений не может быть отклонена, качество модели после преобразования не изменилось. Переходя к логарифму признака, мы рассчитываем сделать данные более нормально распределенными, но, судя по всему, для деревяннных моделей распределение не очень важно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5\n",
    "\n",
    "для числовых признаков построить PCA-признаки, добавить их к основной части датасета.\n",
    "\n",
    "**Комментарий**\n",
    "\n",
    "В EDA было показано наличие большого количества пропусков в датасете. До этого момента пропуски никак не обрабатывались, а PCA не умеет автоматически с ними справляться. Поэтому применю PCA только для полностью заполненных числовых колонок. Кроме того перед применением PCA необходимо нормализовать данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15191777, 0.29229266, 0.40461155, 0.47346957, 0.52940315,\n",
       "       0.5712165 , 0.61146674, 0.6483397 , 0.68041327, 0.70752409,\n",
       "       0.73002076, 0.75127533, 0.7698987 , 0.78685628, 0.80232055,\n",
       "       0.8170429 , 0.83117101, 0.84487214, 0.85797881, 0.87075058,\n",
       "       0.88278623, 0.89346898, 0.90268448, 0.9116765 , 0.9204427 ,\n",
       "       0.92909047, 0.93699095, 0.94375905, 0.94975542, 0.95532819,\n",
       "       0.96037617, 0.96446714, 0.96830648, 0.97172987, 0.9750716 ,\n",
       "       0.97782612, 0.98032974, 0.98244956, 0.98446487, 0.9861024 ,\n",
       "       0.98757124, 0.98899398, 0.99032984, 0.99160604, 0.99282291,\n",
       "       0.99401429, 0.99518718, 0.99604816, 0.99663262, 0.99717724,\n",
       "       0.99768175, 0.99812333, 0.99853717, 0.99892043, 0.99926331,\n",
       "       0.99950245, 0.99969771, 0.99983096, 0.99988913, 0.99992084,\n",
       "       0.99994244, 0.99996101, 0.99997337, 0.99998399, 0.99999149,\n",
       "       0.99999644, 0.99999806, 0.99999957, 0.99999986, 0.99999999,\n",
       "       1.        , 1.        ])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_wo_nan = all_data[all_data.columns[~all_data.isnull().any()]].columns.tolist()\n",
    "numerical = train[columns_wo_nan].drop(columns=['TransactionID', TARGET_COLUMN])\\\n",
    "    .select_dtypes([np.number]).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "all_data_scaled = scaler.fit_transform(all_data[numerical])\n",
    "\n",
    "pca = PCA(n_components=len(numerical))\n",
    "pca.fit(all_data_scaled)\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8884, valid-score = 0.8395\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8846, valid-score = 0.8568\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8774, valid-score = 0.8862\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8821, valid-score = 0.8613\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8822, valid-score = 0.8856\n",
      "=====================================================================\n",
      "CV-results train: 0.8829 +/- 0.004\n",
      "CV-results valid: 0.8659 +/- 0.018\n",
      "OOF-score = 0.8639\n",
      "Public test score: 0.8630\n",
      "OOF_SCORE изменился на 0.0012\n",
      "Public score изменился на 0.0031\n",
      "Ttest: Ttest_relResult(statistic=5.6695947515979075, pvalue=7.525307308400315e-07)\n"
     ]
    }
   ],
   "source": [
    "train_pca = copy.deepcopy(train)\n",
    "public_test_x_pca = copy.deepcopy(public_test_x)\n",
    "\n",
    "data_scaled = scaler.transform(train_pca[numerical])\n",
    "transformed_components = pca.transform(data_scaled)\n",
    "pca_df = pd.DataFrame(data=transformed_components)\n",
    "train_pca = train_pca.join(pca_df)\n",
    "\n",
    "data_scaled = scaler.transform(public_test_x_pca[numerical])\n",
    "transformed_components = pca.transform(data_scaled)\n",
    "pca_df = pd.DataFrame(data=transformed_components)\n",
    "public_test_x_pca = public_test_x_pca.join(pca_df)\n",
    "\n",
    "compare_with_base(train_pca, public_test_x_pca, model, BASE_OOF_SCORE, \n",
    "                  BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "PCA признаки дают хороший значимый прирост"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заданние 6\n",
    "\n",
    "использовать критерий отбора признаков на основе перестановок для отбора признаков, которые положительно влияют на перформанс модели. Переобучить модель и сделать выводы о полученном качестве алгоритма.\n",
    "\n",
    "**Комментарий**\n",
    "\n",
    "Так как в процессе иследования преобразования в датасете отбирались на основе ttest, интересно сравнить качество модели только на \"хороших\" дополнительных признаках с качеством на всех признаках. После этого применю метод пермутированной важности и постараюсь отобрать признаки, чтобы еще больше повысить качество модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(data_train: pd.DataFrame, data_test: pd.DataFrame, \n",
    "                     start_date: datetime.datetime, \n",
    "                     card_features: List, freq_features: List,\n",
    "                     log_transform: bool\n",
    "                     ):\n",
    "    data_train = copy.deepcopy(data_train)\n",
    "    data_test = copy.deepcopy(data_test)\n",
    "    \n",
    "    # признаки на основе даты\n",
    "    data_train = create_date_features(data_train, 'TransactionDT', start_date)\n",
    "    data_test = create_date_features(data_test, 'TransactionDT', start_date)\n",
    "    \n",
    "    # Скомбинированные по данным карты признаки\n",
    "    all_data = pd.concat([data_train, data_test])\n",
    "    aggs = {'TransactionAmt': [np.mean]}\n",
    "    \n",
    "    for feature in card_features:\n",
    "        stats = create_numerical_aggs(all_data, groupby_id=feature, aggs=aggs, suffix=f\"_ON_{feature}\")\n",
    "        stat_name = stats.columns[-1]\n",
    "\n",
    "        data_train = data_train.merge(stats, how='left', on=feature) \n",
    "        data_train[f'TRANSACTIONAMT - {stat_name}'] = data_train['TransactionAmt'] - data_train[stat_name]\n",
    "        data_train[f'TRANSACTIONAMT / {stat_name}'] = data_train['TransactionAmt'] / data_train[stat_name]\n",
    "        \n",
    "        data_test = data_test.merge(stats, how='left', on=feature) \n",
    "        data_test[f'TRANSACTIONAMT - {stat_name}'] = data_test['TransactionAmt'] - data_test[stat_name]\n",
    "        data_test[f'TRANSACTIONAMT / {stat_name}'] = data_test['TransactionAmt'] / data_test[stat_name]\n",
    "    \n",
    "    # Признаки закодированные с помощью частот\n",
    "    all_data = pd.concat([data_train, data_test])\n",
    "    \n",
    "    for feature in freq_features:\n",
    "        for data in [data_train, data_test]:\n",
    "            freq_encoder = all_data[feature].value_counts(normalize=True)\n",
    "            data[f'{feature}_freq_enc'] = data[feature].map(freq_encoder)\n",
    "            data.drop(columns=[feature], inplace=True)\n",
    "    \n",
    "    # преобразование в логарифм признака TransactionAmt\n",
    "    if log_transform:\n",
    "        for data in [data_train, data_test]:\n",
    "            data['TransactionAmt'] = data['TransactionAmt'].apply(np.log)\n",
    "            data.rename(columns={'TransactionAmt': 'LogTransAmt'}, inplace=True)\n",
    "            data['LogTransAmt_frac'] = data['LogTransAmt'].map(lambda x: np.modf(x)[0])\n",
    "            data['LogTransAmt_int'] = data['LogTransAmt'].map(lambda x: np.modf(x)[1])\n",
    "        \n",
    "    # PCA признаки\n",
    "    all_data = pd.concat([data_train, data_test])\n",
    "    columns_wo_nan = all_data[all_data.columns[~all_data.isnull().any()]].columns.tolist()\n",
    "    numerical = all_data[columns_wo_nan].select_dtypes([np.number]).columns.tolist()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    all_data_scaled = scaler.fit_transform(all_data[numerical])\n",
    "\n",
    "    pca = PCA(n_components=len(numerical))\n",
    "    pca.fit(all_data_scaled)\n",
    "    \n",
    "    data_scaled = scaler.transform(data_train[numerical])\n",
    "    transformed_components = pca.transform(data_scaled)\n",
    "    pca_df = pd.DataFrame(data=transformed_components)\n",
    "    data_train = data_train.join(pca_df)\n",
    "    \n",
    "    data_scaled = scaler.transform(data_test[numerical])\n",
    "    transformed_components = pca.transform(data_scaled)\n",
    "    pca_df = pd.DataFrame(data=transformed_components)\n",
    "    data_test = data_test.join(pca_df)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_features_all = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n",
    "freq_features_all = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n",
    "\n",
    "card_features_best = ['card3', 'card4']\n",
    "freq_features_best = ['card1', 'card3', 'card4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all, public_test_x_all = add_new_features(train, public_test_x, start_date, \n",
    "                                                card_features_all, freq_features_all,\n",
    "                                                log_transform=True)\n",
    "\n",
    "train_best, public_test_x_best = add_new_features(train, public_test_x, start_date, \n",
    "                                                  card_features_best, freq_features_best,\n",
    "                                                  log_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8935, valid-score = 0.84\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.887, valid-score = 0.8612\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8795, valid-score = 0.8835\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8862, valid-score = 0.8635\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8843, valid-score = 0.8869\n",
      "=====================================================================\n",
      "CV-results train: 0.8861 +/- 0.005\n",
      "CV-results valid: 0.867 +/- 0.017\n",
      "OOF-score = 0.8651\n",
      "Public test score: 0.8653\n",
      "OOF_SCORE изменился на 0.0024\n",
      "Public score изменился на 0.0054\n",
      "Ttest: Ttest_relResult(statistic=15.73690525026213, pvalue=8.550084992622942e-21)\n"
     ]
    }
   ],
   "source": [
    "compare_with_base(train_all, public_test_x_all, model, BASE_OOF_SCORE, \n",
    "                  BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8897, valid-score = 0.8411\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8817, valid-score = 0.8551\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8769, valid-score = 0.8818\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8837, valid-score = 0.8631\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8839, valid-score = 0.885\n",
      "=====================================================================\n",
      "CV-results train: 0.8832 +/- 0.004\n",
      "CV-results valid: 0.8652 +/- 0.016\n",
      "OOF-score = 0.863\n",
      "Public test score: 0.8656\n",
      "OOF_SCORE изменился на 0.0003\n",
      "Public score изменился на 0.0057\n",
      "Ttest: Ttest_relResult(statistic=3.128037594541308, pvalue=0.00296033306367832)\n"
     ]
    }
   ],
   "source": [
    "compare_with_base(train_best, public_test_x_best, model, BASE_OOF_SCORE, \n",
    "                  BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комментарий**\n",
    "\n",
    "Получилось интересно. Обе модели дают значимый прирост, но при этом, согласно ttest, предсказания первой модели от базовой отличаются амного сильнее. Вторая модель показала незначительно лучший скор на паблике, но при этом меньшее улучшение на OOF выборке. Для дальнейшего исследования и расчета пермутированной важности воспользуюсь данными с полным набором измененных признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_permutation_importance(estimator, \n",
    "                                     metric: callable,\n",
    "                                     x_valid: pd.DataFrame,\n",
    "                                     y_valid: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Расчет пермутированной важности признаков.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    y_pred = estimator.predict_proba(x_valid)[:, 1]\n",
    "    base_score = metric(y_valid, y_pred)\n",
    "\n",
    "    for feature in tqdm(x_valid.columns):\n",
    "        x_valid_copy = x_valid.copy()\n",
    "        x_valid_copy[feature] = np.random.permutation(x_valid_copy[feature])\n",
    "\n",
    "        y_pred = estimator.predict_proba(x_valid_copy)[:, 1]\n",
    "        score = metric(y_valid, y_pred)\n",
    "        scores[feature] = base_score - score\n",
    "\n",
    "    scores = pd.Series(scores)\n",
    "    scores = scores.sort_values(ascending=False)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = train_all.select_dtypes(\"object\").columns.tolist()\n",
    "train_all[cat_features] = train_all[cat_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = train_test_split(\n",
    "    train_all.drop(columns=['TransactionID', TARGET_COLUMN], axis=1), train_size=0.7, random_state=1\n",
    ")\n",
    "y_train, y_valid = train_test_split(\n",
    "    train_all[TARGET_COLUMN], train_size=0.7, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fb2ecd8d390>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, cat_features=cat_features, verbose=0, \n",
    "                      eval_set=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511/511 [01:07<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "perm_importance = calculate_permutation_importance(\n",
    "    estimator=model, metric=roc_auc_score, x_valid=x_valid, y_valid=y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_features = [feature for feature in perm_importance.keys() if perm_importance[feature] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.002926712904809e-07"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_importance['day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 40000, valid-observations = 10001\n",
      "train-score = 0.8912, valid-score = 0.8419\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8847, valid-score = 0.858\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8825, valid-score = 0.8844\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8879, valid-score = 0.8682\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 40001, valid-observations = 10000\n",
      "train-score = 0.8835, valid-score = 0.8871\n",
      "=====================================================================\n",
      "CV-results train: 0.886 +/- 0.003\n",
      "CV-results valid: 0.8679 +/- 0.017\n",
      "OOF-score = 0.866\n",
      "Public test score: 0.8655\n",
      "OOF_SCORE изменился на 0.0033\n",
      "Public score изменился на 0.0056\n",
      "Ttest: Ttest_relResult(statistic=9.485266190050506, pvalue=1.1290934571639186e-12)\n"
     ]
    }
   ],
   "source": [
    "train_filtered = train_all.drop(columns=bad_features)\n",
    "public_test_x_filtered = public_test_x_all.drop(columns=bad_features)\n",
    "\n",
    "compare_with_base(train_filtered, public_test_x_filtered, model, BASE_OOF_SCORE, \n",
    "                  BASE_PUBLIC_SCORE, BASE_SCORES_FOR_TTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОД**\n",
    "\n",
    "после отбрасывания неважных признаков, скор на паблике упал на 0.01%, измененения ввсе так же значимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
